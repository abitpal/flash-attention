{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b39bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b08d9445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/envs/gpudrive/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCFLAGS = -arch=sm_75 -O3 -lineinfo --use_fast_math --keep-device-functions -std=c++17\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faster_attn = load(name='faster_attn', sources=['../src/main.cpp', '../src/flash_attention_kernel.cu'], extra_cuda_cflags=['-O3', '-arch=sm_75', '--use_fast_math'])\n",
    "'''\n",
    "CFLAGS = -arch=sm_75 -O3 -lineinfo --use_fast_math --keep-device-functions -std=c++17\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7de228dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6039ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.timeit(599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2dd6dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(): \n",
    "    B, H, S, D = 4, 8, 128, 64  # Batch, Heads, Seq Len, Head Dim\n",
    "    dtype = torch.float32\n",
    "    device = 'cuda'\n",
    "\n",
    "    q = torch.randn(B, H, S, D, device=device, dtype=dtype).contiguous()\n",
    "    k = torch.randn(B, H, S, D, device=device, dtype=dtype).contiguous()\n",
    "    v = torch.randn(B, H, S, D, device=device, dtype=dtype).contiguous()\n",
    "    return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "37581fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom implementation run: \n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7d26481dda50>\n",
      "f(*args, **kwargs)\n",
      "  84.84 us\n",
      "  1 measurement, 599 runs , 1 thread\n",
      "The memory efficient implementation runs:\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7d2606758610>\n",
      "f(*args, **kwargs)\n",
      "  25.66 us\n",
      "  1 measurement, 599 runs , 1 thread\n",
      "The math implementation run:\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7d2606758650>\n",
      "f(*args, **kwargs)\n",
      "  85.68 us\n",
      "  1 measurement, 599 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from torch.nn import functional as F\n",
    "torch.set_float32_matmul_precision(\"high\")  # ensure it's not \"medium\" or \"highest\" (they allow mixed precision)\n",
    "q, k, v = gen_data()\n",
    "\n",
    "# Flash attention not supported\n",
    "# with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "#     try:\n",
    "#         flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, q, k, v)\n",
    "#         print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n",
    "#     except RuntimeError:\n",
    "#         print(\"FlashAttention is not supported. See warnings for reasons.\")\n",
    "\n",
    "mes=benchmark_torch_function_in_microseconds(faster_attn.forward, q, k, v)\n",
    "print(f\"Custom implementation run: \")\n",
    "print(mes)\n",
    "\n",
    "with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n",
    "    try:\n",
    "        mes=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, q, k, v)\n",
    "        print(f\"The memory efficient implementation runs:\")\n",
    "        print(mes)\n",
    "    except RuntimeError:\n",
    "        print(\"EfficientAttention is not supported. See warnings for reasons.\")\n",
    "\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    mes=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, q, k, v)\n",
    "    print(f\"The math implementation run:\")\n",
    "    print(mes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "214fea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = F.scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f295c4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6d201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpudrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
